{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc747e00-bea3-48cf-bb09-bd3d446bd0a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:26.020163Z",
     "iopub.status.busy": "2025-05-16T16:38:26.019709Z",
     "iopub.status.idle": "2025-05-16T16:38:28.917342Z",
     "shell.execute_reply": "2025-05-16T16:38:28.914874Z",
     "shell.execute_reply.started": "2025-05-16T16:38:26.020122Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go \n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.preprocessing import normalize, LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9055c5db-d8a3-45dc-948f-d3447d061cfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:28.923335Z",
     "iopub.status.busy": "2025-05-16T16:38:28.921766Z",
     "iopub.status.idle": "2025-05-16T16:38:29.125302Z",
     "shell.execute_reply": "2025-05-16T16:38:29.123050Z",
     "shell.execute_reply.started": "2025-05-16T16:38:28.923252Z"
    }
   },
   "outputs": [],
   "source": [
    "# We load the DataFrames \n",
    "\n",
    "# Training data\n",
    "df_sensors = pd.read_csv('nuclear-waste/Coordinates_Training.csv')\n",
    "hum = pd.read_csv('nuclear-waste/Training_data_humidity.csv')\n",
    "pre = pd.read_csv('nuclear-waste/Training_data_pressure.csv')\n",
    "df_tem = pd.read_csv('nuclear-waste/Training_data_temperature.csv')  \n",
    "\n",
    "# Example of submission\n",
    "ex = pd.read_csv('nuclear-waste/example_of_submission.csv')\n",
    "\n",
    "# Test data\n",
    "df_test = pd.read_csv('nuclear-waste/Coordinates_Test.csv')\n",
    "humtest = pd.read_csv('nuclear-waste/Test_Time_humidity.csv')\n",
    "pretest = pd.read_csv('nuclear-waste/Test_Time_pressure.csv')\n",
    "               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359acf7f-766d-4863-9e34-2dd9ff4fc4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With the scatter_3d function from plotly, we will visualize the sensors position in a 3D space\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_sensors,\n",
    "    x='Coor X [m]',\n",
    "    y='Coor Y [m]',\n",
    "    z='Coor Z [m]',\n",
    "    width = 800,\n",
    "    height = 600,\n",
    "    hover_name='Sensor ID' \n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a82c699-352f-4a01-ba25-79d83713d1fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.127910Z",
     "iopub.status.busy": "2025-05-16T16:38:29.127411Z",
     "iopub.status.idle": "2025-05-16T16:38:29.135550Z",
     "shell.execute_reply": "2025-05-16T16:38:29.133639Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.127868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b8c42c7-76a0-4d8e-9089-a7fe45b46328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.139787Z",
     "iopub.status.busy": "2025-05-16T16:38:29.139291Z",
     "iopub.status.idle": "2025-05-16T16:38:29.191216Z",
     "shell.execute_reply": "2025-05-16T16:38:29.189342Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.139734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace with value\n",
    "\n",
    "def replacewithvalue(\n",
    "    df : pd.DataFrame,\n",
    "    header : str,\n",
    "    val = np.nan,\n",
    "    min_val = -float('inf') ,\n",
    "    max_val = float('inf') ,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Replace values between a certain interval with a unique value\n",
    "    Clipping\n",
    "\n",
    "    df : DataFrame\n",
    "    header : Header of column\n",
    "    value : Value used to replace, NaN by default\n",
    "    min_val : minimal value of the interval\n",
    "    max_val : maximal value of the interval\n",
    "    \"\"\"\n",
    "    for i in df[header]:\n",
    "        if min_val < i < max_val and (type(i) == float or type(i) == str) :\n",
    "            df.replace(to_replace = i, value=val, inplace= True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed52f446-f0ed-4aa2-be68-d4971cd40d2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.194718Z",
     "iopub.status.busy": "2025-05-16T16:38:29.194011Z",
     "iopub.status.idle": "2025-05-16T16:38:29.226752Z",
     "shell.execute_reply": "2025-05-16T16:38:29.225171Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.194659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here we will replace NaNs with another value. For example, the arithmetric average or the mean\n",
    "\n",
    "\n",
    "def avgcolumn(\n",
    "    df : pd.DataFrame,\n",
    "    header : str,\n",
    "    min_val = -float('inf') ,\n",
    "    max_val = float('inf') ,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Returns arithmetric average (mean) of a DateFrame column\n",
    "\n",
    "    df : The target DataFrame\n",
    "    header : header of column\n",
    "    min_val : minimal value of the interval\n",
    "    max_val : maximal value of the interval\n",
    "    \"\"\"\n",
    "    liste = []\n",
    "    for i in df[header]:\n",
    "        if min_val < i < max_val and isinstance(i, (int, float))  :\n",
    "            liste.append(i)\n",
    "    if not liste :\n",
    "        return 0\n",
    "    else :\n",
    "        return sum(liste)/len(liste)\n",
    "\n",
    "# P.S. From now on, the 'average' refers to the mean unless stated otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ab8110-1e90-4bf9-94e6-06a5a09f03b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.230173Z",
     "iopub.status.busy": "2025-05-16T16:38:29.228590Z",
     "iopub.status.idle": "2025-05-16T16:38:29.271647Z",
     "shell.execute_reply": "2025-05-16T16:38:29.269839Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.230126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make an average of a feature for a unique sensor\n",
    "\n",
    "def avgsensor(\n",
    "    df: pd.DataFrame, \n",
    "    sensor: str, \n",
    "    feature: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Makes an average of a feature for a unique sensor\n",
    "    df : the target DaataFrame\n",
    "    sensor : the target sensor\n",
    "    feature : the target feature\n",
    "    \"\"\"\n",
    "    filtered = df[df['Sensor ID'] == sensor]\n",
    "\n",
    "    if filtered.empty:\n",
    "        return 0.0\n",
    "        \n",
    "    return filtered[feature].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "581d9dd1-fde5-4185-9fef-e4ec7cf320f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.274409Z",
     "iopub.status.busy": "2025-05-16T16:38:29.273869Z",
     "iopub.status.idle": "2025-05-16T16:38:29.315646Z",
     "shell.execute_reply": "2025-05-16T16:38:29.313421Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.274338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make it so it removes sensors\n",
    "\n",
    "def removesensor(\n",
    "    df: pd.DataFrame, \n",
    "    sensor: str\n",
    ")-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes sensor and it's data from the DataFrame\n",
    "    df : the target DataFrame\n",
    "    sensor : the removed sensor\n",
    "    \"\"\"\n",
    "    df_cleaned = df[df['Sensor ID'] != sensor]\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd2fc80-3542-46bb-8ec5-a3a3352558a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.318849Z",
     "iopub.status.busy": "2025-05-16T16:38:29.317827Z",
     "iopub.status.idle": "2025-05-16T16:38:29.361935Z",
     "shell.execute_reply": "2025-05-16T16:38:29.359174Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.318794Z"
    }
   },
   "outputs": [],
   "source": [
    "def Xy(\n",
    "    fulldf : pd.DataFrame\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes DataFrame and split it into features X and target y (optional)\n",
    "    Returns:\n",
    "    - X, y : if target 'Temperature' column is present\n",
    "    - X : otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = ['M.Time[d]', 'Coor X [m]', 'Coor Y [m]', 'Coor Z [m]', 'R [m]', 'Humidity', 'Pressure']\n",
    "\n",
    "    if 'Material' in fulldf.columns:\n",
    "        X = fulldf[cols + ['Material']]\n",
    "    elif 'Material_encoded' in fulldf.columns:\n",
    "        X = fulldf[cols + ['Material_encoded']]\n",
    "    else:\n",
    "        raise ValueError(\"Missing 'Material' or 'Material_encoded' column in DataFrame.\")\n",
    "    \n",
    "    if 'Temperature' in fulldf.columns : \n",
    "        y = fulldf[['Temperature']]\n",
    "        return X, y\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87be0669-efcd-4c9f-b1b7-211fa94642e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.366263Z",
     "iopub.status.busy": "2025-05-16T16:38:29.365169Z",
     "iopub.status.idle": "2025-05-16T16:38:29.412918Z",
     "shell.execute_reply": "2025-05-16T16:38:29.409999Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.366158Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(\n",
    "    y_val : pd.DataFrame,\n",
    "    y_pred : pd.DataFrame,\n",
    "    model\n",
    "):\n",
    "    \"\"\"\n",
    "    Outputs the Mean Squared Error (MSE) and the coefficient of determination (R²) as well as the importance of features (for xgboost mainly)\n",
    "    y_val : DataFrame of validation data\n",
    "    y_pred : DataFrame of prediction data\n",
    "    model : model\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"MSE : {mse:.4f}\")\n",
    "    print(f\"R² : {r2:.4f}\")\n",
    "    print(f\"Features importances : {model.feature_importances_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d279fa6-c82d-44f5-a938-d4ec48b19f89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.416331Z",
     "iopub.status.busy": "2025-05-16T16:38:29.415992Z",
     "iopub.status.idle": "2025-05-16T16:38:29.455742Z",
     "shell.execute_reply": "2025-05-16T16:38:29.453133Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.416300Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_nan_with_sensor_mean(\n",
    "    row, \n",
    "    header: str, \n",
    "    sensor_means: dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Fill NaNs value with a dictionary of sensor means\n",
    "    row : row\n",
    "    header : which feature to target\n",
    "    sensor_means : dictionary of sensor means\n",
    "    \"\"\"\n",
    "    if pd.isna(row[header]):\n",
    "        return sensor_means.get(row['Sensor ID'], np.nan)\n",
    "    return row[header]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf3cb3c0-23e6-470c-8722-110e96f07752",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.458236Z",
     "iopub.status.busy": "2025-05-16T16:38:29.457698Z",
     "iopub.status.idle": "2025-05-16T16:38:29.500484Z",
     "shell.execute_reply": "2025-05-16T16:38:29.498039Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.458188Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# 1. Make a DataFrame with a column for each feature + label\n",
    "# 2. Add all the features\n",
    "# 3. Treat outliers and missing values\n",
    "# 4. Create X,y\n",
    "# 5. Encode materials into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "418eaf55-d7f0-4306-ba1c-a22c6a9f41a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.504026Z",
     "iopub.status.busy": "2025-05-16T16:38:29.503265Z",
     "iopub.status.idle": "2025-05-16T16:38:29.657720Z",
     "shell.execute_reply": "2025-05-16T16:38:29.655956Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.503956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combining the data on one data frame will facilitate the access to the said data\n",
    "\n",
    "df_long = df_tem.melt(id_vars='M.Time[d]', var_name='Sensor ID', value_name='Temperature')\n",
    "df_merged = df_long.merge(df_sensors[['Sensor ID', 'Index', 'Material','Coor X [m]','Coor Y [m]','Coor Z [m]', 'R [m]']], on='Sensor ID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c622ac-cee1-4ee0-95b1-f081f01b7cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.664085Z",
     "iopub.status.busy": "2025-05-16T16:38:29.663106Z",
     "iopub.status.idle": "2025-05-16T16:38:29.671782Z",
     "shell.execute_reply": "2025-05-16T16:38:29.669846Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.664006Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's apply these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d75a931-267a-4f3d-97cc-dd7678dfcd66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.694002Z",
     "iopub.status.busy": "2025-05-16T16:38:29.693458Z",
     "iopub.status.idle": "2025-05-16T16:38:29.811684Z",
     "shell.execute_reply": "2025-05-16T16:38:29.808865Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.693956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding humidity as a feature\n",
    "df_hum = hum.melt(id_vars='M.Time[d]', var_name='Sensor ID', value_name='Humidity')\n",
    "\n",
    "# Add a column in the main DataFrame\n",
    "df_merged['Humidity'] = df_hum['Humidity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53caa83d-da70-497d-8718-b71951e58bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.817868Z",
     "iopub.status.busy": "2025-05-16T16:38:29.816032Z",
     "iopub.status.idle": "2025-05-16T16:38:29.907181Z",
     "shell.execute_reply": "2025-05-16T16:38:29.906075Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.817787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding pressure as a feature\n",
    "df_pre = pre.melt(id_vars='M.Time[d]', var_name='Sensor ID', value_name='Pressure')\n",
    "\n",
    "# Add a column in the main DataFrame\n",
    "df_merged['Pressure'] = df_pre['Pressure']\n",
    "\n",
    "# We didn't remove potential outliers from either humidity pressure (apart from missing data) as we didn't find a reliable way to find them. \n",
    "# They didn't end up causing any major issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17c8dec2-43bd-45f6-84d8-fc4dda3cf634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:29.942610Z",
     "iopub.status.busy": "2025-05-16T16:38:29.941066Z",
     "iopub.status.idle": "2025-05-16T16:38:30.023791Z",
     "shell.execute_reply": "2025-05-16T16:38:30.022022Z",
     "shell.execute_reply.started": "2025-05-16T16:38:29.942536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M.Time[d]</th>\n",
       "      <th>Sensor ID</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Index</th>\n",
       "      <th>Material</th>\n",
       "      <th>Coor X [m]</th>\n",
       "      <th>Coor Y [m]</th>\n",
       "      <th>Coor Z [m]</th>\n",
       "      <th>R [m]</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14112</th>\n",
       "      <td>1554</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14113</th>\n",
       "      <td>1556</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>1558</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14115</th>\n",
       "      <td>1560</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14116</th>\n",
       "      <td>1563</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14117</th>\n",
       "      <td>1567</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14118</th>\n",
       "      <td>1572</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14119</th>\n",
       "      <td>1578</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14120</th>\n",
       "      <td>1585</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14121</th>\n",
       "      <td>1595</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14122</th>\n",
       "      <td>1606</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14123</th>\n",
       "      <td>1621</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14124</th>\n",
       "      <td>1639</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14125</th>\n",
       "      <td>1662</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14126</th>\n",
       "      <td>1690</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14127</th>\n",
       "      <td>1726</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14128</th>\n",
       "      <td>1770</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14129</th>\n",
       "      <td>1826</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14130</th>\n",
       "      <td>1895</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14131</th>\n",
       "      <td>1982</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14132</th>\n",
       "      <td>2090</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14133</th>\n",
       "      <td>2226</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14134</th>\n",
       "      <td>2395</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14135</th>\n",
       "      <td>2607</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14136</th>\n",
       "      <td>2871</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14137</th>\n",
       "      <td>3202</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14138</th>\n",
       "      <td>3616</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14139</th>\n",
       "      <td>4133</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14140</th>\n",
       "      <td>4779</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14141</th>\n",
       "      <td>5587</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14142</th>\n",
       "      <td>6597</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14143</th>\n",
       "      <td>7028</td>\n",
       "      <td>N_442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442</td>\n",
       "      <td>VOID</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>6.606527</td>\n",
       "      <td>-0.309136</td>\n",
       "      <td>0.553314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       M.Time[d] Sensor ID  Temperature  Index Material  Coor X [m]  \\\n",
       "14112       1554     N_442          NaN    442     VOID   -0.458902   \n",
       "14113       1556     N_442          NaN    442     VOID   -0.458902   \n",
       "14114       1558     N_442          NaN    442     VOID   -0.458902   \n",
       "14115       1560     N_442          NaN    442     VOID   -0.458902   \n",
       "14116       1563     N_442          NaN    442     VOID   -0.458902   \n",
       "14117       1567     N_442          NaN    442     VOID   -0.458902   \n",
       "14118       1572     N_442          NaN    442     VOID   -0.458902   \n",
       "14119       1578     N_442          NaN    442     VOID   -0.458902   \n",
       "14120       1585     N_442          NaN    442     VOID   -0.458902   \n",
       "14121       1595     N_442          NaN    442     VOID   -0.458902   \n",
       "14122       1606     N_442          NaN    442     VOID   -0.458902   \n",
       "14123       1621     N_442          NaN    442     VOID   -0.458902   \n",
       "14124       1639     N_442          NaN    442     VOID   -0.458902   \n",
       "14125       1662     N_442          NaN    442     VOID   -0.458902   \n",
       "14126       1690     N_442          NaN    442     VOID   -0.458902   \n",
       "14127       1726     N_442          NaN    442     VOID   -0.458902   \n",
       "14128       1770     N_442          NaN    442     VOID   -0.458902   \n",
       "14129       1826     N_442          NaN    442     VOID   -0.458902   \n",
       "14130       1895     N_442          NaN    442     VOID   -0.458902   \n",
       "14131       1982     N_442          NaN    442     VOID   -0.458902   \n",
       "14132       2090     N_442          NaN    442     VOID   -0.458902   \n",
       "14133       2226     N_442          NaN    442     VOID   -0.458902   \n",
       "14134       2395     N_442          NaN    442     VOID   -0.458902   \n",
       "14135       2607     N_442          NaN    442     VOID   -0.458902   \n",
       "14136       2871     N_442          NaN    442     VOID   -0.458902   \n",
       "14137       3202     N_442          NaN    442     VOID   -0.458902   \n",
       "14138       3616     N_442          NaN    442     VOID   -0.458902   \n",
       "14139       4133     N_442          NaN    442     VOID   -0.458902   \n",
       "14140       4779     N_442          NaN    442     VOID   -0.458902   \n",
       "14141       5587     N_442          NaN    442     VOID   -0.458902   \n",
       "14142       6597     N_442          NaN    442     VOID   -0.458902   \n",
       "14143       7028     N_442          NaN    442     VOID   -0.458902   \n",
       "\n",
       "       Coor Y [m]  Coor Z [m]     R [m]  Humidity  Pressure  \n",
       "14112    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14113    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14114    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14115    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14116    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14117    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14118    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14119    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14120    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14121    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14122    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14123    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14124    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14125    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14126    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14127    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14128    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14129    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14130    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14131    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14132    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14133    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14134    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14135    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14136    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14137    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14138    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14139    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14140    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14141    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14142    6.606527   -0.309136  0.553314       NaN       NaN  \n",
       "14143    6.606527   -0.309136  0.553314       NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# By scrolling through the training data, we found that N_442 didn't have any temperature values attributed to it \n",
    "display(df_merged[df_merged['Sensor ID']=='N_442'])\n",
    "\n",
    "# So we decided to remove it entirely (Deletion)\n",
    "# Remove N_442\n",
    "df_merged = removesensor(df_merged,'N_442')\n",
    "\n",
    "# Same for N_518 and N_693 but for lack of pressure and humidity data\n",
    "df_merged = removesensor(df_merged,'N_518')\n",
    "df_merged = removesensor(df_merged,'N_693')\n",
    "#df_merged = removesensor(df_merged,'N_891')\n",
    "#df_merged = removesensor(df_merged,'N_892')\n",
    "#df_merged = removesensor(df_merged,'N_893')\n",
    "#df_merged = removesensor(df_merged,'N_894')\n",
    "#df_merged = removesensor(df_merged,'N_895')\n",
    "#df_merged = removesensor(df_merged,'N_896')\n",
    "#df_merged = removesensor(df_merged,'N_897')\n",
    "#df_merged = removesensor(df_merged,'N_898')\n",
    "#df_merged = removesensor(df_merged,'N_898')\n",
    "#df_merged = removesensor(df_merged,'N_899')\n",
    "\n",
    "# The explanation is in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "132dc802-7af5-4fab-8521-e661e92639a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:30.102952Z",
     "iopub.status.busy": "2025-05-16T16:38:30.101674Z",
     "iopub.status.idle": "2025-05-16T16:38:31.931738Z",
     "shell.execute_reply": "2025-05-16T16:38:31.928570Z",
     "shell.execute_reply.started": "2025-05-16T16:38:30.102875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.9373009\n",
      "2518.947148359651\n"
     ]
    }
   ],
   "source": [
    "# We can observe a huge gap in the temperature data, here we print the two values next to where the gap occurs\n",
    "print(df_merged[df_merged['Temperature'] < 1000]['Temperature'].max())\n",
    "print(df_merged[df_merged['Temperature'] > 1000]['Temperature'].min())\n",
    "\n",
    "# We can safely assume that the latter values are outliers. We then remove them and replace them with NaNs\n",
    "replacewithvalue(df_merged, 'Temperature', min_val=1000.0)\n",
    "\n",
    "# Calculating the mean temperature of the sensors with data on it (without outliers!)\n",
    "sensor_means = df_merged.groupby('Sensor ID')['Temperature'].mean()\n",
    "\n",
    "# Filling NaNs with this mean\n",
    "df_merged['Temperature'] = df_merged.apply(lambda row: fill_nan_with_sensor_mean(row, 'Temperature', sensor_means), axis=1)\n",
    "\n",
    "# Since an average isn't a very realistic value for missing data, we want to minimize its presence in our dataset, hence the removal of N_442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a1646ef-ac5f-44d5-b83c-4bf44ef4c64c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:31.941555Z",
     "iopub.status.busy": "2025-05-16T16:38:31.938765Z",
     "iopub.status.idle": "2025-05-16T16:38:31.973115Z",
     "shell.execute_reply": "2025-05-16T16:38:31.971813Z",
     "shell.execute_reply.started": "2025-05-16T16:38:31.941349Z"
    }
   },
   "outputs": [],
   "source": [
    "# We combine the dataset and extract features and target variables\n",
    "X, y = Xy(df_merged)\n",
    "\n",
    "# Renaming the columns for simplicity and clarity\n",
    "X = X.rename(columns={\n",
    "    'M.Time[d]': 'Time',\n",
    "    'Coor X [m]': 'X',\n",
    "    'Coor Y [m]': 'Y',\n",
    "    'Coor Z [m]': 'Z',\n",
    "    'R [m]': 'R'\n",
    "})\n",
    "\n",
    "# We then use the train_test_split function to, in our case, split our data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3fc0782-3c46-4990-a47b-d2e9983ab219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T16:38:31.974694Z",
     "iopub.status.busy": "2025-05-16T16:38:31.974408Z",
     "iopub.status.idle": "2025-05-16T16:38:32.029223Z",
     "shell.execute_reply": "2025-05-16T16:38:32.026948Z",
     "shell.execute_reply.started": "2025-05-16T16:38:31.974668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Material encoder\n",
    "\n",
    "# Label encoder, numeralize the material to make it a feature\n",
    "# This is the first encoder we used, we ended up changing it\n",
    "\n",
    "#le = LabelEncoder()\n",
    "#df_merged['Material_encoded'] = le.fit_transform(df_merged['Material'])\n",
    "\n",
    "# We opted for a target encoding or also called mean encoding\n",
    "# It transforms the material into the mean of the target values (temperature)\n",
    "\n",
    "# material_target_mean will be re-used for test data\n",
    "material_target_mean = X_train.assign(temp=y_train).groupby('Material')['temp'].mean()\n",
    "\n",
    "X_train['Material_encoded'] = X_train['Material'].map(material_target_mean)\n",
    "X_val['Material_encoded'] = X_val['Material'].map(material_target_mean).fillna(y_train.mean())\n",
    "\n",
    "X_train = X_train.drop('Material', axis=1)\n",
    "X_val = X_val.drop('Material', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245d18b-7c6e-4d0d-89fd-5b7a0007e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2a160-1854-49bb-9a19-b6223c7dae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a06d0-f241-402e-ad2e-bec10f50a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tested Linear regression L1, linear regression L2 and KNN Regressor --> Not good"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0598197-a928-4aff-86e1-b30ae8791f6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ed595-afb9-439e-9e6a-d982089a0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor \n",
    "\n",
    "rf = RandomForestRegressor(n_estimators= 85, max_depth= 6, random_state= 13)\n",
    "a = rf.fit(X_train, y_train.values.ravel())\n",
    "y_predrf = rf.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6731a53-b586-4aae-9fb9-68317544217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(y_val, y_predrf, rf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a9dcca3-2f58-4816-9309-2953ac47b5d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:38:04.684901Z",
     "iopub.status.busy": "2025-05-06T14:38:04.684207Z",
     "iopub.status.idle": "2025-05-06T14:38:04.717495Z",
     "shell.execute_reply": "2025-05-06T14:38:04.714712Z",
     "shell.execute_reply.started": "2025-05-06T14:38:04.684840Z"
    }
   },
   "source": [
    "# Random Forest Regressor Conclusion\n",
    "Overfits way more often, is quite faster than other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207689af-8d7a-4e99-8418-fb165966464a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d924b-40b2-4cdc-84f6-4a20b98218f8",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-16T16:39:37.616Z",
     "iopub.execute_input": "2025-05-16T16:38:35.020160Z",
     "iopub.status.busy": "2025-05-16T16:38:35.019315Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:14.17753\tvalidation_1-rmse:13.93508\n",
      "[1]\tvalidation_0-rmse:13.51471\tvalidation_1-rmse:13.28103\n",
      "[2]\tvalidation_0-rmse:12.88740\tvalidation_1-rmse:12.66268\n",
      "[3]\tvalidation_0-rmse:12.28980\tvalidation_1-rmse:12.07356\n",
      "[4]\tvalidation_0-rmse:11.72322\tvalidation_1-rmse:11.51812\n",
      "[5]\tvalidation_0-rmse:11.18481\tvalidation_1-rmse:10.98482\n",
      "[6]\tvalidation_0-rmse:10.67340\tvalidation_1-rmse:10.48250\n",
      "[7]\tvalidation_0-rmse:10.18644\tvalidation_1-rmse:10.00465\n",
      "[8]\tvalidation_0-rmse:9.72523\tvalidation_1-rmse:9.55161\n",
      "[9]\tvalidation_0-rmse:9.28626\tvalidation_1-rmse:9.12254\n",
      "[10]\tvalidation_0-rmse:8.87036\tvalidation_1-rmse:8.71487\n",
      "[11]\tvalidation_0-rmse:8.47616\tvalidation_1-rmse:8.33063\n",
      "[12]\tvalidation_0-rmse:8.09860\tvalidation_1-rmse:7.96326\n",
      "[13]\tvalidation_0-rmse:7.74121\tvalidation_1-rmse:7.61312\n",
      "[14]\tvalidation_0-rmse:7.39980\tvalidation_1-rmse:7.28046\n",
      "[15]\tvalidation_0-rmse:7.07665\tvalidation_1-rmse:6.96701\n",
      "[16]\tvalidation_0-rmse:6.76924\tvalidation_1-rmse:6.66768\n",
      "[17]\tvalidation_0-rmse:6.47854\tvalidation_1-rmse:6.38132\n",
      "[18]\tvalidation_0-rmse:6.19982\tvalidation_1-rmse:6.10994\n",
      "[19]\tvalidation_0-rmse:5.92991\tvalidation_1-rmse:5.84929\n",
      "[20]\tvalidation_0-rmse:5.67500\tvalidation_1-rmse:5.60057\n",
      "[21]\tvalidation_0-rmse:5.43738\tvalidation_1-rmse:5.36913\n",
      "[22]\tvalidation_0-rmse:5.20844\tvalidation_1-rmse:5.14639\n",
      "[23]\tvalidation_0-rmse:4.99351\tvalidation_1-rmse:4.93598\n",
      "[24]\tvalidation_0-rmse:4.78972\tvalidation_1-rmse:4.73871\n",
      "[25]\tvalidation_0-rmse:4.59220\tvalidation_1-rmse:4.54774\n",
      "[26]\tvalidation_0-rmse:4.40928\tvalidation_1-rmse:4.37360\n",
      "[27]\tvalidation_0-rmse:4.23278\tvalidation_1-rmse:4.20112\n",
      "[28]\tvalidation_0-rmse:4.06814\tvalidation_1-rmse:4.04089\n",
      "[29]\tvalidation_0-rmse:3.91253\tvalidation_1-rmse:3.89184\n",
      "[30]\tvalidation_0-rmse:3.76147\tvalidation_1-rmse:3.74710\n",
      "[31]\tvalidation_0-rmse:3.62116\tvalidation_1-rmse:3.61171\n",
      "[32]\tvalidation_0-rmse:3.48845\tvalidation_1-rmse:3.48159\n",
      "[33]\tvalidation_0-rmse:3.36179\tvalidation_1-rmse:3.35965\n",
      "[34]\tvalidation_0-rmse:3.23768\tvalidation_1-rmse:3.24000\n",
      "[35]\tvalidation_0-rmse:3.12594\tvalidation_1-rmse:3.13033\n",
      "[36]\tvalidation_0-rmse:3.01813\tvalidation_1-rmse:3.02738\n",
      "[37]\tvalidation_0-rmse:2.91540\tvalidation_1-rmse:2.92832\n",
      "[38]\tvalidation_0-rmse:2.81906\tvalidation_1-rmse:2.83510\n",
      "[39]\tvalidation_0-rmse:2.72496\tvalidation_1-rmse:2.74491\n",
      "[40]\tvalidation_0-rmse:2.63965\tvalidation_1-rmse:2.66301\n",
      "[41]\tvalidation_0-rmse:2.55758\tvalidation_1-rmse:2.58505\n",
      "[42]\tvalidation_0-rmse:2.48090\tvalidation_1-rmse:2.50961\n",
      "[43]\tvalidation_0-rmse:2.40977\tvalidation_1-rmse:2.44142\n",
      "[44]\tvalidation_0-rmse:2.33973\tvalidation_1-rmse:2.37400\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "\n",
    "# We create the model with defined hyperparameters\n",
    "xgbm = XGBRegressor(n_estimators= 300, max_depth= 6, learning_rate= 0.05, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "\n",
    "# We then train it on the training data\n",
    "xgbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=True)\n",
    "\n",
    "# We now make a prediction with the features of the validation data\n",
    "y_predxgbm = xgbm.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2456c-395e-4f55-8d7d-0546a67cf4fc",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-16T16:39:37.616Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We the compare the prediction with the actual data that we have (y_val)\n",
    "eval(y_val, y_predxgbm, xgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb5d7e-e0ba-442c-bc5a-6dac4fef005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a graph\n",
    "\n",
    "results = xgbm.evals_result()\n",
    "train_rmse = results['validation_0']['rmse']\n",
    "val_rmse = results['validation_1']['rmse']\n",
    "iterations = list(range(len(train_rmse)))\n",
    "\n",
    "# Tracé du graphique\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, train_rmse, label='Training RMSE', marker='o', linewidth=1)\n",
    "plt.plot(iterations, val_rmse, label='Validation RMSE', marker='o', linewidth=1)\n",
    "\n",
    "plt.title('RMSE per iteration (XGBoost)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb5a20-5afa-4a1e-a5d6-3e398c88399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ae034-388d-4f86-b1d5-a76036fe4bee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K-Folds cross-validation\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=7)\n",
    "scores = []\n",
    "\n",
    "xgbm = XGBRegressor(\n",
    "    n_estimators=85,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.09,\n",
    "    random_state=13\n",
    ")\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    material_target_mean2 = X_train.assign(temp=y_train).groupby('Material')['temp'].mean()\n",
    "\n",
    "    X_train['Material_encoded'] = X_train['Material'].map(material_target_mean2)\n",
    "    X_val['Material_encoded'] = X_val['Material'].map(material_target_mean2).fillna(y_train.mean())\n",
    "\n",
    "    X_train = X_train.drop('Material', axis=1)\n",
    "    X_val = X_val.drop('Material', axis=1)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    xgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)],  verbose=True)\n",
    "    y_pred = xgbm.predict(X_val)\n",
    "    score = mean_squared_error(y_val, y_pred)\n",
    "    scores.append(score)\n",
    "    \n",
    "rmse_scores = np.sqrt(scores)\n",
    "    \n",
    "print(\"CV RMSE mean:\", rmse_scores.mean())\n",
    "print(\"CV RMSE std:\", rmse_scores.std())\n",
    "\n",
    "\n",
    "\n",
    "print(\"CV MSE mean:\", np.mean(scores))\n",
    "print(\"CV MSE std:\", np.std(scores))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4d5c6fe-8ed2-4dc5-84a9-503288c529f6",
   "metadata": {},
   "source": [
    "Before final43\n",
    "\n",
    "test_size=0.2\n",
    "\n",
    "MSE : 3.9226\n",
    "R² : 0.9826\n",
    "\n",
    "\n",
    "test_size=0.175\n",
    "\n",
    "MSE : 3.9226\n",
    "R² : 0.9825\n",
    "\n",
    "\n",
    "test_size=0.225\n",
    "MSE : 3.6463\n",
    "R² : 0.9842\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3cb8a7-d0d0-4ae5-811e-3a84d44dc150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "456a8602-7ac8-4785-a374-b225090cea1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "10c307c4-e4f4-49c9-8094-df165e807333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:42:56.304855Z",
     "iopub.status.busy": "2025-05-06T14:42:56.304211Z",
     "iopub.status.idle": "2025-05-06T14:42:56.321229Z",
     "shell.execute_reply": "2025-05-06T14:42:56.318414Z",
     "shell.execute_reply.started": "2025-05-06T14:42:56.304803Z"
    }
   },
   "source": [
    "# XGBoost Conclusion\n",
    "Slow, yields the best results as of final4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934c570-4ed4-4b3c-bc1b-f6a99840eb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4c398-b365-49c5-bf03-5654b9a529cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regression\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.08)\n",
    "gbr.fit(X_train, y_train.values.ravel())\n",
    "y_predgbr = gbr.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e97ae0-754b-4e04-85a2-5052c7b6c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(y_val, y_predgbr, gbr)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f6e5df5-d165-4789-9a9c-ab4ffcc5fd6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1ee7b3dd-c14d-420d-b77c-630491b4d9ce",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921b3be-16da-467a-a571-7f3fdc701174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf9063-679f-4f6c-9029-483185fff562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6edf52e3-9df5-4fbe-a50b-2062d4451701",
   "metadata": {},
   "source": [
    "Comparisons before final4\n",
    "The comparisons down will be made with these hyperparameters :\n",
    "(n_estimators=50, max_depth = 20, learning_rate=0.1, random_state=5)\n",
    "\n",
    "\n",
    "Random Forest Regressor\n",
    "\n",
    "Without Humidity\n",
    "MSE : 9.2585\n",
    "R² : 0.9642\n",
    "\n",
    "With Humidity\n",
    "MSE : 0.0166\n",
    "R² : 0.9999\n",
    "\n",
    "\n",
    "XGBoost\n",
    "\n",
    "Without Humidity\n",
    "MSE : 9.7307\n",
    "R² : 0.9623\n",
    "\n",
    "With Humidity\n",
    "MSE : 0.0090\n",
    "R² : 1.0000\n",
    "\n",
    "\n",
    "Gradient Boosting Regression\n",
    "\n",
    "Without Humidity\n",
    "MSE : 15.8976\n",
    "R² : 0.9385\n",
    "\n",
    "With Humidity\n",
    "MSE : 11.3148\n",
    "R² : 0.9510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9797e7f-c6a3-44c3-8a89-cbf7e1350337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b369a2b-ff35-4904-909a-6a46e159699a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ee328-9aa1-406c-a1cb-d1a38bb8d94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f3db6-2a22-4d65-b9a9-a9d17a5a6de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a91b6-f8c6-49e8-b464-d119afbfd3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bc85f-1378-481b-b8d8-159e142e1606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec697dc-25f0-4cee-ad33-eef9e06f5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81208c-376f-4b87-a9ac-7a3f2ece8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#le2 = LabelEncoder()\n",
    "#df_test['Material_encoded'] = le2.fit_transform(df_test['Material'])\n",
    "\n",
    "# Creating a temporary key feature which will be useful to create a full DataFrame similar to df_merged\n",
    "df_test[\"key\"] = 1\n",
    "\n",
    "# Extracting the time stamps from df_tem to a separate temporary DataFrame\n",
    "df_times = pd.DataFrame({\"M.Time[d]\": df_tem['M.Time[d]']})\n",
    "\n",
    "# Adding the same temporary key feature\n",
    "df_times[\"key\"] = 1\n",
    "\n",
    "# Perform a cartesian merge to replicate the structure of df_merged (without the temperatures)\n",
    "df_test_prepared = pd.merge(df_test, df_times, on=\"key\").drop(\"key\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10938f-403f-497b-b3a3-24705da66027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding humidity as a feature\n",
    "df_humtest = humtest.melt(id_vars='M.Time[d]', var_name='Sensor ID', value_name='Humidity')\n",
    "\n",
    "# Adding it to the main DataFrame\n",
    "df_test_prepared['Humidity'] = df_humtest['Humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275d141-2939-4617-806d-f77011c37df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding pressure as a feature\n",
    "df_pretest = pretest.melt(id_vars='M.Time[d]', var_name='Sensor ID', value_name='Pressure')\n",
    "\n",
    "# Adding it to the main DataFrame\n",
    "df_test_prepared['Pressure'] = df_pretest['Pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85dafde-2bc0-40cb-b74d-c9e5f192d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Material encoder\n",
    "df_test_prepared['Material_encoded'] = df_test_prepared['Material'].map(material_target_mean).fillna(y_train.mean())\n",
    "df_test_prepared = df_test_prepared.drop('Material', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da49e9-9022-4adb-a328-616238af1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feature dataframe (picking only the useful features)\n",
    "X_test = Xy(df_test_prepared)\n",
    "\n",
    "# Renaming the features\n",
    "X_test = X_test.rename(columns={\n",
    "    'M.Time[d]': 'Time',  \n",
    "    'Coor X [m]': 'X',\n",
    "    'Coor Y [m]': 'Y',\n",
    "    'Coor Z [m]': 'Z',\n",
    "    'R [m]': 'R'\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4bb6a-6e1e-498f-840d-71c6a28223ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the temperatures with the predict function\n",
    "\n",
    "# MODIFY MODEL \n",
    "         # HERE \n",
    "        #   v\n",
    "y_pred_t = xgbm.predict(X_test)\n",
    "\n",
    "# Reshape the predictions\n",
    "y_pred_t = y_pred_t.reshape(int(4640/32),32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5a81d-a1a9-423f-be08-0c9e3605b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column names from the 'M.Time[d]' values\n",
    "header = df_tem['M.Time[d]'].to_numpy()\n",
    "\n",
    "# Recategorize them to string\n",
    "header = header.astype(str)\n",
    "\n",
    "# Retyping header to a list \n",
    "header = list(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244c6fb-89da-4ed9-bbe3-e1d6980cea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sensor ids from submission example and converting it to numpy array\n",
    "ids = ex['id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195679c3-8966-43d0-a4cc-da7a306e916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the final DataFrame with the predictions\n",
    "final = pd.DataFrame(y_pred_t, columns=header)\n",
    "\n",
    "# Finally, inserting the sensor ids in the first column\n",
    "final.insert(0, \"id\", ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c6e3a-aa42-47be-b547-60381555e3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de2475-4b53-4e28-8eb8-c7a52773ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the final DataFrame to .csv\n",
    "final.to_csv(\"Results/final72.csv\", index=False)\n",
    "\n",
    "# DONE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "791c9825-c94c-49b1-80c4-20543a1ec263",
   "metadata": {},
   "source": [
    "### Has (X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "final4, score: 62.10919 : xgbm = XGBRegressor(n_estimators=50, max_depth=20, learning_rate=0.1, random_state=5), Validation : MSE: 0.0090 (overfitting)\n",
    "\n",
    "### From now on, validation MSE will always be lower than the test score (presumably MSE), we are dealing with overfitting. \n",
    "\n",
    "final5, score: 43.81606 : xgbm = XGBRegressor(n_estimators=100, max_depth=10, learning_rate=0.05, random_state=5)\n",
    "final6, score: 31.69807 : xgbm = XGBRegressor(n_estimators=80, max_depth=8, learning_rate=0.05, random_state=5)\n",
    "final7, score: 29.69597 : xgbm = XGBRegressor(n_estimators=80, max_depth=8, learning_rate=0.08, random_state=5)\n",
    "final8, score: 55.95477 : xgbm = XGBRegressor(n_estimators=120, max_depth=12, learning_rate=0.1, random_state=5)\n",
    "final9, score: 30.13921 : xgbm = XGBRegressor(n_estimators=60, max_depth=8, learning_rate=0.08, random_state=5)\n",
    "final10, score: 29.76974 : xgbm = XGBRegressor(n_estimators=100, max_depth=8, learning_rate=0.08, random_state=5)\n",
    "\n",
    "### Removed N_442\n",
    "\n",
    "final11, score: 82.03570 : gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.08)\n",
    "\n",
    "final12, score: 34.68443 : xgbm = XGBRegressor(n_estimators=80, max_depth=8, learning_rate=0.08, random_state=5) \n",
    "\n",
    "\n",
    "\n",
    "### Added Pressure without removing potential outliers\n",
    "\n",
    "final13, score: 36.67656 : xgbm = XGBRegressor(n_estimators=80, max_depth=8, learning_rate=0.08, random_state=5) \n",
    "\n",
    "\n",
    "\n",
    "### Added Z-Score standardization\n",
    "\n",
    "final14, score: 423.48151 : xgbm = XGBRegressor(n_estimators=80, max_depth=8, learning_rate=0.08, random_state=5) \n",
    "\n",
    "\n",
    "\n",
    "### Removed Z-Score standardization\n",
    "\n",
    "final15, score: 325.61693 : xgbm = XGBRegressor(n_estimators= 80, max_depth= 6, learning_rate= 0.09720768452782377, subsample= 0.764080771313621, colsample_bytree= 0.780194180355509, reg_alpha= 0.016637986915063695, reg_lambda = 3.663118344084126, min_child_weight = 5)\n",
    "final16, score: 325.61693 : xgbm = XGBRegressor(n_estimators= 120, max_depth= 12, learning_rate= 0.08, random_state= 5)\n",
    "\n",
    "\n",
    "\n",
    "### Removed Material\n",
    "\n",
    "final17, score: 26.56924 : xgbm = XGBRegressor(n_estimators= 80, max_depth= 10, learning_rate= 0.08, random_state= 5)\n",
    "\n",
    "\n",
    "\n",
    "### Removed Humidity\n",
    "\n",
    "final18, score: 29.60394 : xgbm = XGBRegressor(n_estimators= 80, max_depth= 10, learning_rate= 0.08, random_state= 5)\n",
    "\n",
    "\n",
    "\n",
    "### Added back Humidity and material with different material encoder\n",
    "### That's probably when the duplicated rows error occured while tweaking code\n",
    "\n",
    "final19, score: 26.64930 : xgbm = XGBRegressor(n_estimators= 80, max_depth= 10, learning_rate= 0.08, random_state= 5)\n",
    "final20, score: 32.60856 : xgbm = XGBRegressor(n_estimators= 90, max_depth= 12, learning_rate= 0.08, random_state= 5)\n",
    "final21, score: 26.68753 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 10, learning_rate= 0.08, random_state= 5)\n",
    "final22, score: 24.53315 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 8, learning_rate= 0.08, random_state= 5)\n",
    "final23, score: 17.04642 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 6, learning_rate= 0.09, random_state= 5)\n",
    "final24, score: 25.20745 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 5, learning_rate= 0.09, random_state= 5)\n",
    "\n",
    "### Identified optimal max_depth = 6\n",
    "\n",
    "final25, score: 17.15851 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 6, learning_rate= 0.11, random_state= 5)\n",
    "final26, score: 27.58055 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 6, learning_rate= 0.7, random_state= 5)\n",
    "final27, score: 19.89351 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 6, learning_rate= 0.07, random_state= 5)\n",
    "final28, score: 16.68647 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 6)\n",
    "\n",
    "### Identified potential optimal learning_rate = 0.09\n",
    "\n",
    "final29, score: 20.26213 : xgbm = XGBRegressor(n_estimators= 90, max_depth= 6, learning_rate= 0.085, random_state= 6)\n",
    "final30, score: 16.96652 : xgbm = XGBRegressor(n_estimators= 95, max_depth= 6, learning_rate= 0.09, random_state= 6)\n",
    "final31, score: 18.84561 : xgbm = XGBRegressor(n_estimators= 100, max_depth= 6, learning_rate= 0.08, random_state= 6)\n",
    "final32, score: 18.84561 : xgbm = XGBRegressor(n_estimators= 90, max_depth= 6, learning_rate= 0.095, random_state= 6)\n",
    "\n",
    "### Optimal hyperparameters : (n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 6)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5e5b791-2da6-4e67-a2a9-24f7a426eb7b",
   "metadata": {},
   "source": [
    "### Trying more hyperparameters\n",
    "final33, score: 19.58426 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, subsample = 0.9, random_state= 6)\n",
    "final34, score: 18.28214 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, colsample_bytree=0.95, random_state= 6)\n",
    "\n",
    "### New hyperparameters yield no additional performances\n",
    "### Added standardization of Pressure and Humidity\n",
    "\n",
    "final35, score: 17.74830 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 6)\n",
    "\n",
    "### Tweaked features (removed redundant hum pre)\n",
    "\n",
    "final36, score: 17.60165 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 6)\n",
    "\n",
    "### Removed standardization\n",
    "### Fixed a problem with duplicated rows which was in the training model since god knows how long\n",
    "\n",
    "final37, score: 7.61592 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 6) Validation : MSE : 3.9226 (slight overfit ?)\n",
    "\n",
    "### The model now does a pretty good job at generalizing\n",
    "\n",
    "### Confirmation, tweaking the hyperparameters\n",
    "\n",
    "final38, score: 9.42210 : xgbm = XGBRegressor(n_estimators= 88, max_depth= 6, learning_rate= 0.088, random_state= 13)\n",
    "final39, score: 8.35975 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.091, random_state= 13)\n",
    "final40, score: 8.18224 : xgbm = XGBRegressor(n_estimators= 82, max_depth= 6, learning_rate= 0.089, random_state= 13)\n",
    "final41, score: 13.54004 : xgbm = XGBRegressor(n_estimators= 95, max_depth= 7, learning_rate= 0.09, random_state= 13)\n",
    "final42, score: 13.54004 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 5, learning_rate= 0.09, random_state= 13) (probably a mistake)\n",
    "\n",
    "\n",
    "### Changing test/val subsets\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.175, random_state=13)\n",
    "final43, score: 9.12317 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13)\n",
    "\n",
    "\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.225, random_state=13)\n",
    "final44, score: 8.65197 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13)\n",
    "\n",
    "\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.225, random_state=6)\n",
    "final45, score: 6.37966 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state=13)\n",
    "\n",
    "\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=6)\n",
    "final46, score: 6.06178 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state=13)\n",
    "\n",
    "\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=6)\n",
    "final47, score: 8.88391 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state=13)\n",
    "\n",
    "\n",
    "### train_test_split(X, y, test_size=0.25, random_state=6), seems to be optimal\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=7)\n",
    "final48, score: 5.95466 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state=13)\n",
    "CV RMSE mean: 2.2122849629030084\n",
    "CV RMSE std: 0.5128506309014986\n",
    "CV MSE mean: 5.157220526702832\n",
    "CV MSE std: 2.2932409742263835\n",
    "\n",
    "### So :\n",
    "### train_test_split(X, y, test_size=0.25, random_state=7), seems to be optimal\n",
    "### XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state=13)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5d8a80f-7d61-487e-ac7c-f76734e61dcf",
   "metadata": {},
   "source": [
    "### Applied mean per sensor and removed N_854, N_899\n",
    "\n",
    "final49, score: 6.38396 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, subsample=0.75)\n",
    "final50, score: 5.98776 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13)\n",
    "CV RMSE mean: 1.209076390278342\n",
    "CV RMSE std: 0.05065649352120937\n",
    "CV MSE mean: 1.4644317978643695\n",
    "CV MSE std: 0.12158664306626986\n",
    "final51, score: 5.93405 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 5, learning_rate= 0.09, random_state= 13)\n",
    "final52, score: 7.16625 : xgbm = XGBRegressor(n_estimators= 60, max_depth= 5, learning_rate= 0.09, random_state= 13)\n",
    "\n",
    "### Removed N_518, N_693\n",
    "final53, score: 9.10019 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13)\n",
    "\n",
    "### Added back N_854, N_899\n",
    "final54, score: 5.12421 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13)\n",
    "final55, score: 4.67788 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5) \n",
    "final56, score: 5.93910 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=2)\n",
    "final57, score: 4.53698 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5)\n",
    "final58, score: 4.77686 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, subsample= 0.9)\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=7)\n",
    "final59, score: 6.48981 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5)\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=7)\n",
    "final60, score: 4.33271 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "final61, score: 4.33271 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.75)\n",
    "final62, score: 5.29234 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.08, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "final63, score: 4.33271 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 0, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=17)\n",
    "final64, score: 6.92225 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 0, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "\n",
    "This indicates that our training data is still flawed\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.26, random_state=7)\n",
    "final65, score: 4.92420 : xgbm = XGBRegressor(n_estimators = 85, max_depth= 6, learning_rate= 0.09, random_state= 0, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.24, random_state=7)\n",
    "final66, score: 6.06455 : xgbm = XGBRegressor(n_estimators = 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=7)\n",
    "final67, score: 6.47727 : xgbm = XGBRegressor(n_estimators = 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=7)\n",
    "final68, score: 4.18491 : xgbm = XGBRegressor(n_estimators = 150, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "\n",
    "We are able to crank up the n_estimators\n",
    "\n",
    "final69, score: 4.14967 : xgbm = XGBRegressor(n_estimators = 300, max_depth= 6, learning_rate= 0.05, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "final70, score: 4.44968 : xgbm = XGBRegressor(n_estimators = 1000, max_depth= 6, learning_rate= 0.025, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "\n",
    "### Remove 891 896 899\n",
    "final71, score: 7.01448 : xgbm = XGBRegressor(n_estimators = 300, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "final72, score: 6.85279 : xgbm = XGBRegressor(n_estimators = 300, max_depth= 6, learning_rate= 0.05, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "DONE\n",
    "\n",
    "\n",
    "Chose 68 and 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdba4b-42f3-40b3-903f-b8ab7351d420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2547a-5857-40d6-b912-96cbfb67a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Results/final48.csv\", \"r\", encoding=\"utf-8\") as f1, open(\"Results/val.csv\", \"r\", encoding=\"utf-8\") as f2:\n",
    "    lignes1 = f1.readlines()\n",
    "    lignes2 = f2.readlines()\n",
    "\n",
    "if lignes1 == lignes2:\n",
    "    print(\"Les fichiers sont identiques.\")\n",
    "else:\n",
    "    print(\"Les fichiers sont différents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8e14b-fb63-4d3f-a06d-fad1d6f8b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger les deux fichiers CSV\n",
    "df1 = pd.read_csv(\"Results/final48.csv\")\n",
    "df2 = pd.read_csv(\"Results/val.csv\")\n",
    "\n",
    "# Trier les colonnes et les lignes\n",
    "df1_sorted = df1.sort_index(axis=1).sort_values(by=df1.columns.tolist()).reset_index(drop=True)\n",
    "df2_sorted = df2.sort_index(axis=1).sort_values(by=df2.columns.tolist()).reset_index(drop=True)\n",
    "\n",
    "# Comparer\n",
    "identiques = df1_sorted.equals(df2_sorted)\n",
    "print(\"Les fichiers sont identiques.\" if identiques else \"Les fichiers sont différents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50089aa-926d-4149-a2a6-6059a7c7a26a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
