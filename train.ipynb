{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc747e00-bea3-48cf-bb09-bd3d446bd0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go \n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.preprocessing import normalize, LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055c5db-d8a3-45dc-948f-d3447d061cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the DataFrames \n",
    "\n",
    "# Training data\n",
    "df_sensors = pd.read_csv('nuclear-waste/Coordinates_Training.csv')\n",
    "hum = pd.read_csv('nuclear-waste/Training_data_humidity.csv')\n",
    "pre = pd.read_csv('nuclear-waste/Training_data_pressure.csv')\n",
    "df_tem = pd.read_csv('nuclear-waste/Training_data_temperature.csv')  \n",
    "\n",
    "# Example of submission\n",
    "ex = pd.read_csv('nuclear-waste/example_of_submission.csv')\n",
    "\n",
    "# Test data\n",
    "df_test = pd.read_csv('nuclear-waste/Coordinates_Test.csv')\n",
    "humtest = pd.read_csv('nuclear-waste/Test_Time_humidity.csv')\n",
    "pretest = pd.read_csv('nuclear-waste/Test_Time_pressure.csv')\n",
    "               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359acf7f-766d-4863-9e34-2dd9ff4fc4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With the scatter_3d function from plotly, we will visualize the sensors position in a 3D space\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_sensors,\n",
    "    x='Coor X [m]',\n",
    "    y='Coor Y [m]',\n",
    "    z='Coor Z [m]',\n",
    "    width = 800,\n",
    "    height = 600,\n",
    "    hover_name='Sensor ID' \n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82c699-352f-4a01-ba25-79d83713d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c42c7-76a0-4d8e-9089-a7fe45b46328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with value\n",
    "\n",
    "def replacewithvalue(\n",
    "    df : pd.DataFrame,\n",
    "    header : str,\n",
    "    val = np.nan,\n",
    "    min_val = -float('inf') ,\n",
    "    max_val = float('inf') ,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Replace values between a certain interval with a unique value\n",
    "    Clipping\n",
    "\n",
    "    df : DataFrame\n",
    "    header : Header of column\n",
    "    value : Value used to replace, NaN by default\n",
    "    min_val : minimal value of the interval\n",
    "    max_val : maximal value of the interval\n",
    "    \"\"\"\n",
    "    for i in df[header]:\n",
    "        if min_val < i < max_val and (type(i) == float or type(i) == str) :\n",
    "            df.replace(to_replace = i, value=val, inplace= True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52f446-f0ed-4aa2-be68-d4971cd40d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will replace NaNs with another value. For example, the arithmetric average or the mean\n",
    "\n",
    "\n",
    "def avgcolumn(\n",
    "    df : pd.DataFrame,\n",
    "    header : str,\n",
    "    min_val = -float('inf') ,\n",
    "    max_val = float('inf') ,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Returns arithmetric average (mean) of a DateFrame column\n",
    "\n",
    "    df : The target DataFrame\n",
    "    header : Header of column\n",
    "    min_val : minimal value of the interval\n",
    "    max_val : maximal value of the interval\n",
    "    \"\"\"\n",
    "    liste = []\n",
    "    for i in df[header]:\n",
    "        if min_val < i < max_val and isinstance(i, (int, float))  :\n",
    "            liste.append(i)\n",
    "    if not liste :\n",
    "        return 0\n",
    "    else :\n",
    "        return sum(liste)/len(liste)\n",
    "\n",
    "# P.S. From now on, the 'average' refers to the mean unless stated otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab8110-1e90-4bf9-94e6-06a5a09f03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgsensor(df: pd.DataFrame, sensor: str, header: str) -> float:\n",
    "    \n",
    "    filtered = df[df['Sensor ID'] == sensor]\n",
    "\n",
    "    if filtered.empty:\n",
    "        return 0.0\n",
    "        \n",
    "    return filtered[header].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d9dd1-fde5-4185-9fef-e4ec7cf320f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it so it removes sensors\n",
    "\n",
    "def removesensor(\n",
    "    df: pd.DataFrame, \n",
    "    sensor: str\n",
    ")-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes sensor and it's data from the DataFrame\n",
    "    df : The target DataFrame\n",
    "    sensor : The removed sensor\n",
    "    \"\"\"\n",
    "    df_cleaned = df[df['Sensor ID'] != sensor]\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2fc80-3542-46bb-8ec5-a3a3352558a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xy(\n",
    "    fulldf : pd.DataFrame\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes DataFrame and split it into features X and target y (optional)\n",
    "    Returns:\n",
    "    - X, y : if target 'Temperature' column is present\n",
    "    - X : otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = ['M.Time[d]', 'Coor X [m]', 'Coor Y [m]', 'Coor Z [m]', 'R [m]', 'Humidity', 'Pressure']\n",
    "\n",
    "    if 'Material' in fulldf.columns:\n",
    "        X = fulldf[cols + ['Material']]\n",
    "    elif 'Material_encoded' in fulldf.columns:\n",
    "        X = fulldf[cols + ['Material_encoded']]\n",
    "    else:\n",
    "        raise ValueError(\"Missing 'Material' or 'Material_encoded' column in DataFrame.\")\n",
    "    \n",
    "    if 'Temperature' in fulldf.columns : \n",
    "        y = fulldf[['Temperature']]\n",
    "        return X, y\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be0669-efcd-4c9f-b1b7-211fa94642e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(\n",
    "    y_val : pd.DataFrame,\n",
    "    y_pred : pd.DataFrame,\n",
    "    model\n",
    "):\n",
    "    \"\"\"\n",
    "    Outputs the Mean Squared Error (MSE) and the coefficient of determination (R²) as well as the importance of features (for xgboost mainly)\n",
    "    y_val : DataFrame of validation data\n",
    "    y_pred : DataFrame of prediction data\n",
    "    model : model\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"MSE : {mse:.4f}\")\n",
    "    print(f\"R² : {r2:.4f}\")\n",
    "    print(f\"Features importances : {model.feature_importances_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d279fa6-c82d-44f5-a938-d4ec48b19f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_with_sensor_mean(\n",
    "    row, \n",
    "    header: str, \n",
    "    sensor_means: dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Fill NaNs value with a dictionary of sensor means\n",
    "    row : row\n",
    "    header : which feature to target\n",
    "    sensor_means : dictionary of sensor means\n",
    "    \"\"\"\n",
    "    if pd.isna(row[header]):\n",
    "        return sensor_means.get(row['Sensor ID'], np.nan)\n",
    "    return row[header]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3cb3c0-23e6-470c-8722-110e96f07752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# 1. Make a DataFrame with a column for each feature + label\n",
    "# 2. Add all the features\n",
    "# 3.\n",
    "# 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418eaf55-d7f0-4306-ba1c-a22c6a9f41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the data on one data frame will facilitate the access to the said data\n",
    "\n",
    "df_long = df_tem.melt(id_vars='M.Time[d]', var_name='Sensor ID', value_name='Temperature')\n",
    "df_merged = df_long.merge(df_sensors[['Sensor ID', 'Index', 'Material','Coor X [m]','Coor Y [m]','Coor Z [m]', 'R [m]']], on='Sensor ID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c622ac-cee1-4ee0-95b1-f081f01b7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75a931-267a-4f3d-97cc-dd7678dfcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding humidity as a feature\n",
    "df_hum = hum.melt(id_vars='M.Time[d]', var_name='Sensor ID', value_name='Humidity')\n",
    "\n",
    "# Add a column in the main DataFrame\n",
    "df_merged['Humidity'] = df_hum['Humidity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53caa83d-da70-497d-8718-b71951e58bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding pressure as a feature\n",
    "df_pre = pre.melt(id_vars='M.Time[d]', var_name='Sensor ID', value_name='Pressure')\n",
    "\n",
    "# Add a column in the main DataFrame\n",
    "df_merged['Pressure'] = df_pre['Pressure']\n",
    "\n",
    "# We didn't remove potential outliers from either humidity pressure (apart from missing data) as we didn't find a reliable way to find them. \n",
    "# They didn't end up causing any major issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8dec2-43bd-45f6-84d8-fc4dda3cf634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By scrolling through the training data, we found that N_442 didn't have any temperature values attributed to it \n",
    "display(df_merged[df_merged['Sensor ID']=='N_442'])\n",
    "\n",
    "# So we decided to remove it entirely (Deletion)\n",
    "# Remove N_442\n",
    "df_merged = removesensor(df_merged,'N_442')\n",
    "\n",
    "# Same for N_518 and N_693 but for lack of pressure and humidity data\n",
    "df_merged = removesensor(df_merged,'N_518')\n",
    "df_merged = removesensor(df_merged,'N_693')\n",
    "\n",
    "# The explanation is in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132dc802-7af5-4fab-8521-e661e92639a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can observe a huge gap in the temperature data, here we print the two values next to where the gap occurs\n",
    "print(df_merged[df_merged['Temperature'] < 1000]['Temperature'].max())\n",
    "print(df_merged[df_merged['Temperature'] > 1000]['Temperature'].min())\n",
    "\n",
    "# We can safely assume that the latter values are outliers. We then remove them and replace them with NaNs\n",
    "replacewithvalue(df_merged, 'Temperature', min_val=1000.0)\n",
    "\n",
    "# Calculating the mean temperature of the sensors with data on it (without outliers!)\n",
    "sensor_means = df_merged.groupby('Sensor ID')['Temperature'].mean()\n",
    "\n",
    "# Filling NaNs with this mean\n",
    "df_merged['Temperature'] = df_merged.apply(lambda row: fill_nan_with_sensor_mean(row, 'Temperature', sensor_means), axis=1)\n",
    "\n",
    "# Since an average isn't a very realistic value for missing data, we want to minimize its presence in our dataset, hence the removal of N_442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1646ef-ac5f-44d5-b83c-4bf44ef4c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We combine the dataset and extract features and target variables\n",
    "X, y = Xy(df_merged)\n",
    "\n",
    "# Renaming the columns for simplicity and clarity\n",
    "X = X.rename(columns={\n",
    "    'M.Time[d]': 'Time',\n",
    "    'Coor X [m]': 'X',\n",
    "    'Coor Y [m]': 'Y',\n",
    "    'Coor Z [m]': 'Z',\n",
    "    'R [m]': 'R'\n",
    "})\n",
    "\n",
    "# We then use the train_test_split function to, in our case, split our data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc0782-3c46-4990-a47b-d2e9983ab219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Material encoder\n",
    "\n",
    "# Label encoder, numeralize the material to make it a feature\n",
    "# This is the first encoder we used, we ended up changing it\n",
    "\n",
    "#le = LabelEncoder()\n",
    "#df_merged['Material_encoded'] = le.fit_transform(df_merged['Material'])\n",
    "\n",
    "# material_target_mean will be re-used for test data\n",
    "material_target_mean = X_train.assign(temp=y_train).groupby('Material')['temp'].mean()\n",
    "\n",
    "X_train['Material_encoded'] = X_train['Material'].map(material_target_mean)\n",
    "X_val['Material_encoded'] = X_val['Material'].map(material_target_mean).fillna(y_train.mean())\n",
    "\n",
    "X_train = X_train.drop('Material', axis=1)\n",
    "X_val = X_val.drop('Material', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245d18b-7c6e-4d0d-89fd-5b7a0007e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2a160-1854-49bb-9a19-b6223c7dae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a06d0-f241-402e-ad2e-bec10f50a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tested Linear regression L1, linear regression L2 and KNN Regressor --> Not good"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0598197-a928-4aff-86e1-b30ae8791f6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ed595-afb9-439e-9e6a-d982089a0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor \n",
    "\n",
    "rf = RandomForestRegressor(n_estimators= 85, max_depth= 6, random_state= 13)\n",
    "a = rf.fit(X_train, y_train.values.ravel())\n",
    "y_predrf = rf.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6731a53-b586-4aae-9fb9-68317544217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(y_val, y_predrf, rf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a9dcca3-2f58-4816-9309-2953ac47b5d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:38:04.684901Z",
     "iopub.status.busy": "2025-05-06T14:38:04.684207Z",
     "iopub.status.idle": "2025-05-06T14:38:04.717495Z",
     "shell.execute_reply": "2025-05-06T14:38:04.714712Z",
     "shell.execute_reply.started": "2025-05-06T14:38:04.684840Z"
    }
   },
   "source": [
    "# Random Forest Regressor Conclusion\n",
    "Overfits way more often, is quite faster than other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207689af-8d7a-4e99-8418-fb165966464a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d924b-40b2-4cdc-84f6-4a20b98218f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 0, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "xgbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=True)\n",
    "y_predxgbm = xgbm.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2456c-395e-4f55-8d7d-0546a67cf4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval(y_val, y_predxgbm, xgbm)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ae034-388d-4f86-b1d5-a76036fe4bee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K-Folds cross-validation\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=7)\n",
    "scores = []\n",
    "\n",
    "xgbm = XGBRegressor(\n",
    "    n_estimators=85,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.09,\n",
    "    random_state=13\n",
    ")\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    material_target_mean2 = X_train.assign(temp=y_train).groupby('Material')['temp'].mean()\n",
    "\n",
    "    X_train['Material_encoded'] = X_train['Material'].map(material_target_mean2)\n",
    "    X_val['Material_encoded'] = X_val['Material'].map(material_target_mean2).fillna(y_train.mean())\n",
    "\n",
    "    X_train = X_train.drop('Material', axis=1)\n",
    "    X_val = X_val.drop('Material', axis=1)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    xgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)],  verbose=True)\n",
    "    y_pred = xgbm.predict(X_val)\n",
    "    score = mean_squared_error(y_val, y_pred)\n",
    "    scores.append(score)\n",
    "    \n",
    "rmse_scores = np.sqrt(scores)\n",
    "    \n",
    "print(\"CV RMSE mean:\", rmse_scores.mean())\n",
    "print(\"CV RMSE std:\", rmse_scores.std())\n",
    "\n",
    "\n",
    "\n",
    "print(\"CV MSE mean:\", np.mean(scores))\n",
    "print(\"CV MSE std:\", np.std(scores))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4d5c6fe-8ed2-4dc5-84a9-503288c529f6",
   "metadata": {},
   "source": [
    "Before final43\n",
    "\n",
    "test_size=0.2\n",
    "\n",
    "MSE : 3.9226\n",
    "R² : 0.9826\n",
    "\n",
    "\n",
    "test_size=0.175\n",
    "\n",
    "MSE : 3.9226\n",
    "R² : 0.9825\n",
    "\n",
    "\n",
    "test_size=0.225\n",
    "MSE : 3.6463\n",
    "R² : 0.9842\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3cb8a7-d0d0-4ae5-811e-3a84d44dc150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "456a8602-7ac8-4785-a374-b225090cea1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "10c307c4-e4f4-49c9-8094-df165e807333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T14:42:56.304855Z",
     "iopub.status.busy": "2025-05-06T14:42:56.304211Z",
     "iopub.status.idle": "2025-05-06T14:42:56.321229Z",
     "shell.execute_reply": "2025-05-06T14:42:56.318414Z",
     "shell.execute_reply.started": "2025-05-06T14:42:56.304803Z"
    }
   },
   "source": [
    "# XGBoost Conclusion\n",
    "Slow, yields the best results as of final4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934c570-4ed4-4b3c-bc1b-f6a99840eb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4c398-b365-49c5-bf03-5654b9a529cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regression\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.08)\n",
    "gbr.fit(X_train, y_train.values.ravel())\n",
    "y_predgbr = gbr.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e97ae0-754b-4e04-85a2-5052c7b6c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(y_val, y_predgbr, gbr)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f6e5df5-d165-4789-9a9c-ab4ffcc5fd6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1ee7b3dd-c14d-420d-b77c-630491b4d9ce",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921b3be-16da-467a-a571-7f3fdc701174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf9063-679f-4f6c-9029-483185fff562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6edf52e3-9df5-4fbe-a50b-2062d4451701",
   "metadata": {},
   "source": [
    "Comparisons before final4\n",
    "The comparisons down will be made with these hyperparameters :\n",
    "(n_estimators=50, max_depth = 20, learning_rate=0.1, random_state=5)\n",
    "\n",
    "\n",
    "Random Forest Regressor\n",
    "\n",
    "Without Humidity\n",
    "MSE : 9.2585\n",
    "R² : 0.9642\n",
    "\n",
    "With Humidity\n",
    "MSE : 0.0166\n",
    "R² : 0.9999\n",
    "\n",
    "\n",
    "XGBoost\n",
    "\n",
    "Without Humidity\n",
    "MSE : 9.7307\n",
    "R² : 0.9623\n",
    "\n",
    "With Humidity\n",
    "MSE : 0.0090\n",
    "R² : 1.0000\n",
    "\n",
    "\n",
    "Gradient Boosting Regression\n",
    "\n",
    "Without Humidity\n",
    "MSE : 15.8976\n",
    "R² : 0.9385\n",
    "\n",
    "With Humidity\n",
    "MSE : 11.3148\n",
    "R² : 0.9510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9797e7f-c6a3-44c3-8a89-cbf7e1350337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b369a2b-ff35-4904-909a-6a46e159699a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ee328-9aa1-406c-a1cb-d1a38bb8d94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f3db6-2a22-4d65-b9a9-a9d17a5a6de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a91b6-f8c6-49e8-b464-d119afbfd3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bc85f-1378-481b-b8d8-159e142e1606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec697dc-25f0-4cee-ad33-eef9e06f5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81208c-376f-4b87-a9ac-7a3f2ece8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#le2 = LabelEncoder()\n",
    "#df_test['Material_encoded'] = le2.fit_transform(df_test['Material'])\n",
    "\n",
    "# Creating a temporary key feature which will be useful to create a full DataFrame similar to df_merged\n",
    "df_test[\"key\"] = 1\n",
    "\n",
    "# Extracting the time stamps from df_tem to a separate temporary DataFrame\n",
    "df_times = pd.DataFrame({\"M.Time[d]\": df_tem['M.Time[d]']})\n",
    "\n",
    "# Adding the same temporary key feature\n",
    "df_times[\"key\"] = 1\n",
    "\n",
    "# Perform a cartesian merge to replicate the structure of df_merged\n",
    "df_test_prepared = pd.merge(df_test, df_times, on=\"key\").drop(\"key\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10938f-403f-497b-b3a3-24705da66027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding humidity as a feature\n",
    "df_humtest = humtest.melt(id_vars='M.Time[d]', var_name='Sensor ID', value_name='Humidity')\n",
    "\n",
    "# Adding it to the main DataFrame\n",
    "df_test_prepared['Humidity'] = df_humtest['Humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275d141-2939-4617-806d-f77011c37df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding pressure as a feature\n",
    "df_pretest = pretest.melt(id_vars='M.Time[d]', var_name='Sensor ID', value_name='Pressure')\n",
    "\n",
    "# Adding it to the main DataFrame\n",
    "df_test_prepared['Pressure'] = df_pretest['Pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85dafde-2bc0-40cb-b74d-c9e5f192d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Material encoder\n",
    "df_test_prepared['Material_encoded'] = df_test_prepared['Material'].map(material_target_mean).fillna(y_train.mean())\n",
    "df_test_prepared = df_test_prepared.drop('Material', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da49e9-9022-4adb-a328-616238af1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feature dataframe (picking only the useful features)\n",
    "X_test = Xy(df_test_prepared)\n",
    "\n",
    "# Renaming the features\n",
    "X_test = X_test.rename(columns={\n",
    "    'M.Time[d]': 'Time',  \n",
    "    'Coor X [m]': 'X',\n",
    "    'Coor Y [m]': 'Y',\n",
    "    'Coor Z [m]': 'Z',\n",
    "    'R [m]': 'R'\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4bb6a-6e1e-498f-840d-71c6a28223ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the temperatures with the predict function\n",
    "\n",
    "# MODIFY MODEL \n",
    "         # HERE \n",
    "        #   v\n",
    "y_pred_t = xgbm.predict(X_test)\n",
    "\n",
    "# Reshape the predictions\n",
    "y_pred_t = y_pred_t.reshape(int(4640/32),32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5a81d-a1a9-423f-be08-0c9e3605b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column names from the 'M.Time[d]' values\n",
    "header = df_tem['M.Time[d]'].to_numpy()\n",
    "\n",
    "# Retyping them to string\n",
    "header = header.astype(str)\n",
    "\n",
    "# Retyping header to a list \n",
    "header = list(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244c6fb-89da-4ed9-bbe3-e1d6980cea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sensor ids from submission example and converting it to numpy array\n",
    "ids = ex['id'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195679c3-8966-43d0-a4cc-da7a306e916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the final DataFrame with the predictions\n",
    "final = pd.DataFrame(y_pred_t, columns=header)\n",
    "\n",
    "# Finally, inserting the sensor ids in the first column\n",
    "final.insert(0, \"id\", ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c6e3a-aa42-47be-b547-60381555e3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de2475-4b53-4e28-8eb8-c7a52773ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to .csv\n",
    "final.to_csv(\"Results/final63.csv\", index=False)\n",
    "\n",
    "# DONE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "791c9825-c94c-49b1-80c4-20543a1ec263",
   "metadata": {},
   "source": [
    "### Has (X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "final4, score: 62.10919 : xgbm = XGBRegressor(n_estimators=50, max_depth=20, learning_rate=0.1, random_state=5), Validation : MSE: 0.0090 (overfitting)\n",
    "\n",
    "### From now on, validation MSE will always be lower than the test score (presumably MSE), we are dealing with overfitting. \n",
    "\n",
    "final5, score: 43.81606 : xgbm = XGBRegressor(n_estimators=100, max_depth=10, learning_rate=0.05, random_state=5)\n",
    "final6, score: 31.69807 : xgbm = XGBRegressor(n_estimators=80, max_depth=8, learning_rate=0.05, random_state=5)\n",
    "final7, score: 29.69597 : xgbm = XGBRegressor(n_estimators=80, max_depth=8, learning_rate=0.08, random_state=5)\n",
    "final8, score: 55.95477 : xgbm = XGBRegressor(n_estimators=120, max_depth=12, learning_rate=0.1, random_state=5)\n",
    "final9, score: 30.13921 : xgbm = XGBRegressor(n_estimators=60, max_depth=8, learning_rate=0.08, random_state=5)\n",
    "final10, score: 29.76974 : xgbm = XGBRegressor(n_estimators=100, max_depth=8, learning_rate=0.08, random_state=5)\n",
    "\n",
    "### Removed N_442\n",
    "\n",
    "final11, score: 82.03570 : gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.08)\n",
    "\n",
    "final12, score: 34.68443 : xgbm = XGBRegressor(n_estimators=80, max_depth=8, learning_rate=0.08, random_state=5) \n",
    "\n",
    "\n",
    "\n",
    "### Added Pressure without removing potential outliers\n",
    "\n",
    "final13, score: 36.67656 : xgbm = XGBRegressor(n_estimators=80, max_depth=8, learning_rate=0.08, random_state=5) \n",
    "\n",
    "\n",
    "\n",
    "### Added Z-Score standardization\n",
    "\n",
    "final14, score: 423.48151 : xgbm = XGBRegressor(n_estimators=80, max_depth=8, learning_rate=0.08, random_state=5) \n",
    "\n",
    "\n",
    "\n",
    "### Removed Z-Score standardization\n",
    "\n",
    "final15, score: 325.61693 : xgbm = XGBRegressor(n_estimators= 80, max_depth= 6, learning_rate= 0.09720768452782377, subsample= 0.764080771313621, colsample_bytree= 0.780194180355509, reg_alpha= 0.016637986915063695, reg_lambda = 3.663118344084126, min_child_weight = 5)\n",
    "final16, score: 325.61693 : xgbm = XGBRegressor(n_estimators= 120, max_depth= 12, learning_rate= 0.08, random_state= 5)\n",
    "\n",
    "\n",
    "\n",
    "### Removed Material\n",
    "\n",
    "final17, score: 26.56924 : xgbm = XGBRegressor(n_estimators= 80, max_depth= 10, learning_rate= 0.08, random_state= 5)\n",
    "\n",
    "\n",
    "\n",
    "### Removed Humidity\n",
    "\n",
    "final18, score: 29.60394 : xgbm = XGBRegressor(n_estimators= 80, max_depth= 10, learning_rate= 0.08, random_state= 5)\n",
    "\n",
    "\n",
    "\n",
    "### Added back Humidity and material with different material encoder\n",
    "### That's probably when the duplicated rows error occured while tweaking code\n",
    "\n",
    "final19, score: 26.64930 : xgbm = XGBRegressor(n_estimators= 80, max_depth= 10, learning_rate= 0.08, random_state= 5)\n",
    "final20, score: 32.60856 : xgbm = XGBRegressor(n_estimators= 90, max_depth= 12, learning_rate= 0.08, random_state= 5)\n",
    "final21, score: 26.68753 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 10, learning_rate= 0.08, random_state= 5)\n",
    "final22, score: 24.53315 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 8, learning_rate= 0.08, random_state= 5)\n",
    "final23, score: 17.04642 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 6, learning_rate= 0.09, random_state= 5)\n",
    "final24, score: 25.20745 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 5, learning_rate= 0.09, random_state= 5)\n",
    "\n",
    "### Identified optimal max_depth = 6\n",
    "\n",
    "final25, score: 17.15851 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 6, learning_rate= 0.11, random_state= 5)\n",
    "final26, score: 27.58055 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 6, learning_rate= 0.7, random_state= 5)\n",
    "final27, score: 19.89351 : xgbm = XGBRegressor(n_estimators= 75, max_depth= 6, learning_rate= 0.07, random_state= 5)\n",
    "final28, score: 16.68647 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 6)\n",
    "\n",
    "### Identified potential optimal learning_rate = 0.09\n",
    "\n",
    "final29, score: 20.26213 : xgbm = XGBRegressor(n_estimators= 90, max_depth= 6, learning_rate= 0.085, random_state= 6)\n",
    "final30, score: 16.96652 : xgbm = XGBRegressor(n_estimators= 95, max_depth= 6, learning_rate= 0.09, random_state= 6)\n",
    "final31, score: 18.84561 : xgbm = XGBRegressor(n_estimators= 100, max_depth= 6, learning_rate= 0.08, random_state= 6)\n",
    "final32, score: 18.84561 : xgbm = XGBRegressor(n_estimators= 90, max_depth= 6, learning_rate= 0.095, random_state= 6)\n",
    "\n",
    "### Optimal hyperparameters : (n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 6)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5e5b791-2da6-4e67-a2a9-24f7a426eb7b",
   "metadata": {},
   "source": [
    "### Trying more hyperparameters\n",
    "final33, score: 19.58426 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, subsample = 0.9, random_state= 6)\n",
    "final34, score: 18.28214 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, colsample_bytree=0.95, random_state= 6)\n",
    "\n",
    "### New hyperparameters yield no additional performances\n",
    "### Added standardization of Pressure and Humidity\n",
    "\n",
    "final35, score: 17.74830 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 6)\n",
    "\n",
    "### Tweaked features (removed redundant hum pre)\n",
    "\n",
    "final36, score: 17.60165 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 6)\n",
    "\n",
    "### Removed standardization\n",
    "### Fixed a problem with duplicated rows which was in the training model since god knows how long\n",
    "\n",
    "final37, score: 7.61592 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 6) Validation : MSE : 3.9226 (slight overfit ?)\n",
    "\n",
    "### The model now does a pretty good job at generalizing\n",
    "\n",
    "### Confirmation, tweaking the hyperparameters\n",
    "\n",
    "final38, score: 9.42210 : xgbm = XGBRegressor(n_estimators= 88, max_depth= 6, learning_rate= 0.088, random_state= 13)\n",
    "final39, score: 8.35975 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.091, random_state= 13)\n",
    "final40, score: 8.18224 : xgbm = XGBRegressor(n_estimators= 82, max_depth= 6, learning_rate= 0.089, random_state= 13)\n",
    "final41, score: 13.54004 : xgbm = XGBRegressor(n_estimators= 95, max_depth= 7, learning_rate= 0.09, random_state= 13)\n",
    "final42, score: 13.54004 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 5, learning_rate= 0.09, random_state= 13) (probably a mistake)\n",
    "\n",
    "\n",
    "### Changing test/val subsets\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.175, random_state=13)\n",
    "final43, score: 9.12317 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13)\n",
    "\n",
    "\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.225, random_state=13)\n",
    "final44, score: 8.65197 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13)\n",
    "\n",
    "\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.225, random_state=6)\n",
    "final45, score: 6.37966 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state=13)\n",
    "\n",
    "\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=6)\n",
    "final46, score: 6.06178 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state=13)\n",
    "\n",
    "\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=6)\n",
    "final47, score: 8.88391 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state=13)\n",
    "\n",
    "\n",
    "### train_test_split(X, y, test_size=0.25, random_state=6), seems to be optimal\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=7)\n",
    "final48, score: 5.95466 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state=13)\n",
    "CV RMSE mean: 2.2122849629030084\n",
    "CV RMSE std: 0.5128506309014986\n",
    "CV MSE mean: 5.157220526702832\n",
    "CV MSE std: 2.2932409742263835\n",
    "\n",
    "### So :\n",
    "### train_test_split(X, y, test_size=0.25, random_state=7), seems to be optimal\n",
    "### XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state=13)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5d8a80f-7d61-487e-ac7c-f76734e61dcf",
   "metadata": {},
   "source": [
    "### Applied mean per sensor and removed N_854, N_899\n",
    "\n",
    "final49, score: 6.38396 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, subsample=0.75)\n",
    "final50, score: 5.98776 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13)\n",
    "CV RMSE mean: 1.209076390278342\n",
    "CV RMSE std: 0.05065649352120937\n",
    "CV MSE mean: 1.4644317978643695\n",
    "CV MSE std: 0.12158664306626986\n",
    "final51, score: 5.93405 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 5, learning_rate= 0.09, random_state= 13)\n",
    "final52, score: 7.16625 : xgbm = XGBRegressor(n_estimators= 60, max_depth= 5, learning_rate= 0.09, random_state= 13)\n",
    "\n",
    "### Removed N_518, N_693\n",
    "final53, score: 9.10019 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13)\n",
    "\n",
    "### Added back N_854, N_899\n",
    "final54, score: 5.12421 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13)\n",
    "final55, score: 4.67788 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5) \n",
    "final56, score: 5.93910 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=2)\n",
    "final57, score: 4.53698 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5)\n",
    "final58, score: 4.77686 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, subsample= 0.9)\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=6)\n",
    "final59, score: 6.48981 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5)\n",
    "\n",
    "### changed X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=6)\n",
    "final60, score: 4.33271 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "final61, score: 4.33271 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.75)\n",
    "final62, score: 5.29234 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.08, random_state= 13, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "final63, score: 4.33271 : xgbm = XGBRegressor(n_estimators= 85, max_depth= 6, learning_rate= 0.09, random_state= 0, reg_lambda=1.5, reg_alpha=0.5, min_child_weight=1.25)\n",
    "\n",
    "DONE\n",
    "\n",
    "Chose 63 and 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdba4b-42f3-40b3-903f-b8ab7351d420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2547a-5857-40d6-b912-96cbfb67a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Results/final48.csv\", \"r\", encoding=\"utf-8\") as f1, open(\"Results/val.csv\", \"r\", encoding=\"utf-8\") as f2:\n",
    "    lignes1 = f1.readlines()\n",
    "    lignes2 = f2.readlines()\n",
    "\n",
    "if lignes1 == lignes2:\n",
    "    print(\"Les fichiers sont identiques.\")\n",
    "else:\n",
    "    print(\"Les fichiers sont différents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8e14b-fb63-4d3f-a06d-fad1d6f8b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger les deux fichiers CSV\n",
    "df1 = pd.read_csv(\"Results/final48.csv\")\n",
    "df2 = pd.read_csv(\"Results/val.csv\")\n",
    "\n",
    "# Trier les colonnes et les lignes\n",
    "df1_sorted = df1.sort_index(axis=1).sort_values(by=df1.columns.tolist()).reset_index(drop=True)\n",
    "df2_sorted = df2.sort_index(axis=1).sort_values(by=df2.columns.tolist()).reset_index(drop=True)\n",
    "\n",
    "# Comparer\n",
    "identiques = df1_sorted.equals(df2_sorted)\n",
    "print(\"Les fichiers sont identiques.\" if identiques else \"Les fichiers sont différents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50089aa-926d-4149-a2a6-6059a7c7a26a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
